{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIpyR7i-XtMy"
      },
      "source": [
        "~~~\n",
        "Copyright 2024 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://colab.research.google.com/github/google-health/cxr-foundation/blob/master/notebooks/retrieve_images_by_text.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"\u003e\u003cbr\u003e Run in Google Colab\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/google-health/cxr-foundation/blob/master/notebooks/retrieve_images_by_text.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://huggingface.co/google/cxr-foundation\"\u003e\n",
        "      \u003cimg alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"\u003e\u003cbr\u003e View on HuggingFace\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTTGbllChDdY"
      },
      "source": [
        "# Text-Based Image Retrieval with Chest X-Ray Embeddings\n",
        "\n",
        "This notebook demonstrates how to use pre-computed embeddings from chest X-ray\n",
        "images for text-based image retrieval. It showcases:\n",
        "\n",
        "- Loading 2737 pre-computed embeddings and labels derived from a subset of the NIH Chest X-ray14 dataset.\n",
        "- Performing a text-based search to find matching images using ELIXR-B embeddings.\n",
        "\n",
        "The embeddings are the *elixr_img_contrastive* which are text-aligned image embedding from the Q-former output in ELIXR (https://arxiv.org/abs/2308.01317), can be used for image retrieval.\n",
        "\n",
        "**NOTE:**  To streamline this Colab demonstration and eliminate the need for lengthy downloads, we've precomputed the embeddings, which are considerably smaller in size (similar to compressed images). You can learn how to generate embeddings using the other [notebooks](https://colab.research.google.com/github/google-health/cxr-foundation/blob/master/notebooks/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authenticate to Access Data"
      ],
      "metadata": {
        "id": "axuHyjBaxWva"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2zwPufcyyl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate with HuggingFace, skip if you have a HF_TOKEN secret\n",
        "\n",
        "# Authenticate user for HuggingFace if needed. Enter token below if requested.\n",
        "from huggingface_hub.utils import HfFolder\n",
        "\n",
        "if HfFolder.get_token() is None:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "else:\n",
        "    print(\"Token already set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evn9WNBSd-CP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Download precomupted embeddings and labels from HuggingFace\n",
        "import pandas as pd\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "HF_REPO_ID = \"google/cxr-foundation\"\n",
        "\n",
        "# Download image thumbnails.\n",
        "THUMBNAILS_FILE_PATH = hf_hub_download(repo_id=HF_REPO_ID, filename='thumbnails_id_to_webp.npz', subfolder='precomputed_embeddings')\n",
        "\n",
        "# Download precomputed embeddings.\n",
        "EMBEDDINGS_NPZ_FILE_PATH = hf_hub_download(repo_id=HF_REPO_ID, filename='embeddings.npz', subfolder='precomputed_embeddings')\n",
        "\n",
        "# Download precomputed text embeddings.\n",
        "TEXT_EMBEDDINGS_NPZ_FILE_PATH = hf_hub_download(repo_id=HF_REPO_ID, filename='text_embeddings.npz', subfolder='precomputed_embeddings')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data Preparation and Similarity Functions\n",
        "import numpy as np\n",
        "\n",
        "# Load files\n",
        "embeddings_file = np.load(EMBEDDINGS_NPZ_FILE_PATH)\n",
        "image_embeddings_df = pd.DataFrame(\n",
        "    [(key, embeddings_file[key]) for key in embeddings_file.keys()],\n",
        "    columns=['image_id', 'embeddings']\n",
        ")\n",
        "embeddings_file.close()\n",
        "\n",
        "# Load text embeddings\n",
        "text_embeddings = np.load(TEXT_EMBEDDINGS_NPZ_FILE_PATH)\n",
        "text_embeddings_queries = list(text_embeddings.keys())\n",
        "\n",
        "# Load images\n",
        "thumbnails = np.load(THUMBNAILS_FILE_PATH, allow_pickle=True)\n",
        "\n",
        "def restructure_embeddings_for_search(df):\n",
        "    \"\"\"Restructures the DataFrame so each image ID has 32 rows, one per sub-vector, with pre-computed norms.\"\"\"\n",
        "    expanded_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        image_id = row['image_id']\n",
        "        reshaped_emb = np.reshape(row['embeddings'], (32, 128))\n",
        "        norms = np.linalg.norm(reshaped_emb, axis=1)\n",
        "        for i in range(32):\n",
        "            expanded_rows.append({\n",
        "                'image_id': image_id,\n",
        "                'sub_vector': reshaped_emb[i],\n",
        "                'norm': norms[i]\n",
        "            })\n",
        "    return pd.DataFrame(expanded_rows)\n",
        "\n",
        "\n",
        "preprocessed_image_embeddings_df = restructure_embeddings_for_search(image_embeddings_df)\n",
        "\n",
        "def find_top_5_similarities_flattened(df_embeddings, txt_emb):\n",
        "    \"\"\"Retrieves the top 5 most similar images to the given text embeddings.\n",
        "\n",
        "    Calculates cosine similarity between image and text embeddings using a flattened\n",
        "    DataFrame for efficient search.\n",
        "    \"\"\"\n",
        "    def calculate_similarity(row, txt_emb):\n",
        "        \"\"\"Calculate similarity between an image embedding and a text embedding.\"\"\"\n",
        "        txt_norm = np.linalg.norm(txt_emb)\n",
        "        return np.dot(row['sub_vector'], txt_emb) / (row['norm'] * txt_norm)\n",
        "\n",
        "    # Calculate similarities for the given text embedding\n",
        "    df_embeddings['similarity'] = df_embeddings.apply(\n",
        "        lambda row: calculate_similarity(row, txt_emb), axis=1\n",
        "    )\n",
        "\n",
        "    # Find the max similarity for each image_id\n",
        "    max_similarities = df_embeddings.groupby('image_id')['similarity'].max().reset_index()\n",
        "\n",
        "    # Sort and get the top 5\n",
        "    top_5 = max_similarities.sort_values(by='similarity', ascending=False).head(5)\n",
        "\n",
        "    return top_5"
      ],
      "metadata": {
        "id": "sJHxx4cvxswM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Retrieval Demo\n",
        "\n",
        "This this demo we use take one or two text queries, fetch EXLIR-B embeddings and\n",
        "use these embeddings to measure similarities between text and images embeddings.\n",
        "Those align textual and visual representations, which is ideal for cross-modal retrieval tasks. We display up to\n",
        "5 top matching images of the subset of the 2737 images that were precached.\n",
        "\n",
        "Please note that the search database is small and covers several diseases."
      ],
      "metadata": {
        "id": "HBXqkQweh6ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Perform Query\n",
        "from ipywidgets import widgets, Layout\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "clear_button = widgets.Button(description=\"Clear\")\n",
        "def clear_results(button):\n",
        "  clear_output()  # Clear the previous output\n",
        "  text_input.value = ''  # Reset the text input widget\n",
        "\n",
        "clear_button.on_click(clear_results)\n",
        "\n",
        "# Create the text input widget with auto-complete\n",
        "text_input = widgets.Combobox(\n",
        "    placeholder='Type query...',\n",
        "    description='Query',\n",
        "    options=text_embeddings_queries,\n",
        "    ensure_option=True  # Ensures that the typed value is in the options\n",
        ")\n",
        "\n",
        "display(text_input)\n",
        "\n",
        "\n",
        "def on_text_change(change):\n",
        "  if change['type'] == 'change' and change['name'] == 'value':\n",
        "    clear_output(wait=True)\n",
        "    selected_query = change['new']\n",
        "    if selected_query:\n",
        "      display(widgets.HBox([text_input, clear_button]))\n",
        "    else:\n",
        "      display(text_input)\n",
        "    filtered_options = [option for option in text_embeddings_queries if selected_query in option]\n",
        "\n",
        "    if len(filtered_options) == 1:\n",
        "      text_input.value = filtered_options[0]  # Set the value to the single option\n",
        "      selected_query = filtered_options[0]\n",
        "\n",
        "    if selected_query in text_embeddings_queries:\n",
        "      print(f\"Selected query: {text_input.value}\")\n",
        "      # Retrieve the text embedding vector using selected_query as the key\n",
        "      out = find_top_5_similarities_flattened(preprocessed_image_embeddings_df, text_embeddings[text_input.value])\n",
        "\n",
        "      for _, row in out.iterrows():\n",
        "        image_id = str(row['image_id'])\n",
        "\n",
        "        # Get the image bytes from thumbnails\n",
        "        image_bytes = thumbnails[image_id]\n",
        "\n",
        "        # Create widgets for image and score\n",
        "        image_widget = widgets.Image(value=image_bytes.tobytes(), format='webp')\n",
        "\n",
        "        # Create a horizontal box to display image and score\n",
        "        hbox = widgets.HBox([\n",
        "            image_widget,\n",
        "            widgets.VBox([\n",
        "                widgets.Label(value=f\"Similarity Score: {row['similarity']:.4f}\"),\n",
        "                widgets.Label(value=f\"Image ID: {row['image_id']}\"),\n",
        "                ], layout=Layout(margin='0px 0px 0px 0px'))])\n",
        "\n",
        "        # Display the image\n",
        "        display(hbox)\n",
        "\n",
        "\n",
        "text_input.observe(on_text_change, names='value')"
      ],
      "metadata": {
        "id": "TDBTMs2si5jl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U1bVpWcfE-H9"
      },
      "cell_type": "markdown",
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/cxr-foundation/blob/master/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3ac608b8f9188be2227ae82298dfd5de684cbdc4496f362d4b3b9040509447c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
